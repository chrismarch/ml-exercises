{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77153405",
   "metadata": {},
   "source": [
    "# Chapter 8 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097f07a",
   "metadata": {},
   "source": [
    "1. _What are the main motivations for reducing a dataset’s dimensionality? What are the main drawbacks?_<br>\n",
    "<br>\n",
    "You can reduce dimensions to visualize. You can also reduce dimensions to simplify the model needed for the data, or perhaps to speed up training.\n",
    "However, you may lose prediction power if information is lost<br>\n",
    "<br>\n",
    "1. _What is the curse of dimensionality?_<br>\n",
    "<br>\n",
    "Higher dimensional spases tend to have sparser datasets, so extrapolations are larger, so models tend to overfit compared to lower dimensality<br><br>\n",
    "1. _Once a dataset’s dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?_<br>\n",
    "<br>\n",
    "It is only possible if the two features were combined in an independent way, such as all possible combinations of two classes<br><br>\n",
    "1. _Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?_<br>\n",
    "<br>\n",
    "Yes, although it is more likely that information will be lost, as the less linear the dataset, the more information will be lost, as the dataset will not fit as well to the principal components.\n",
    "<br><br>\n",
    "1. _Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. How many dimensions will the resulting dataset have?_<br>\n",
    "<br>\n",
    "It will depend on the dataset, and will be determined during training of the PCA.\n",
    "<br>\n",
    "<br><br>\n",
    "1. _In what cases would you use regular PCA, Incremental PCA, Randomized PCA, or Random Projection?_<br>\n",
    "<br>\n",
    "randomized: when the number of features is large enough for training time to be a concern<br>\n",
    "incremental: when the training set doesn't fit in memory, or you want to do online training<br>\n",
    "random projection: if you must reduce to a large number of dimensions, on the order of ten thousand<br>\n",
    "regular: if the cases above do not apply<br>\n",
    "<br><br>\n",
    "1. _How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?_<br>\n",
    "<br>\n",
    "It depends on your goals for the dimensionality reduction. One set of measures would be to compare the speed and predictive performance of models with and without the dimensionality reduction. Another would be to compare the mean squared distance between instances before reduction and after a reduction and reconstruction, to measure how much information is lost.\n",
    "<br><br>\n",
    "1. _Does it make any sense to chain two different dimensionality reduction algorithms?_<br>\n",
    "<br>\n",
    "Yes, the dataset may have large scale structure best served by one algorithm, whose output may have structure better served by another algorithm\n",
    "<br><br>\n",
    "1. _Load the MNIST dataset (introduced in Chapter 3) and split it into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing). Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set. Next, use PCA to reduce the dataset’s dimensionality, with an explained variance ratio of 95%. Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster? Next, evaluate the classifier on the test set. How does it compare to the previous classifier? Try again with an SGDClassifier. How much does PCA help now?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090fbb08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01065ff7",
   "metadata": {},
   "source": [
    "10. _Use t-SNE to reduce the first 5,000 images of the MNIST dataset down to two dimensions and plot the result using Matplotlib. You can use a scatterplot using 10 different colors to represent each image’s target class. Alternatively, you can replace each dot in the scatterplot with the corresponding instance’s class (a digit from 0 to 9), or even plot scaled-down versions of the digit images themselves (if you plot all digits, the visualization will be too cluttered, so you should either draw a random sample or plot an instance only if no other instance has already been plotted at a close distance). You should get a nice visualization with well-separated clusters of digits. Try using other dimensionality reduction algorithms such as PCA, LLE, or MDS and compare the resulting visualizations._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7169a06a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77153405",
   "metadata": {},
   "source": [
    "# Chapter 11 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097f07a",
   "metadata": {},
   "source": [
    "1. _What is the problem that Glorot initialization and He initialization aim to fix?_<br>\n",
    "<br>\n",
    "vanishing/exploding gradients<br>\n",
    "<br><br>\n",
    "1. _Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?_<br>\n",
    "<br>\n",
    "no<br>\n",
    "<br><br>\n",
    "1. _Is it OK to initialize the bias terms to 0?_<br>\n",
    "<br>\n",
    "yes<br>\n",
    "<br><br>\n",
    "1. _In which cases would you want to use each of the activation functions we discussed in this chapter?_<br>\n",
    "<br>\n",
    " * ReLU: good default for simple tasks<br>\n",
    " * Swish: can be good default for complex tasks<br>\n",
    " * leaky ReLU: complex tasks where you care about runtime latency<br>\n",
    " * SELU: deep MLPs when you can meet the constraints\n",
    "<br><br>\n",
    "1. _What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?_<br><br>\n",
    "it won't converge, and will oscillate<br>\n",
    "<br><br>\n",
    "1. _Name three ways you can produce a sparse model._<br>\n",
    "<br>\n",
    " * zero weights under a threshold after training (but this won't make it particularly sparse or performant)<br>\n",
    " * use strong l1 regularization during training<br>\n",
    " * try TF-MOT (optimization toolkit)\n",
    "<br><br>\n",
    "1. _Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?_<br>\n",
    " * training: yes, a little<br>\n",
    " * inference: no, it is disabled<br>\n",
    " * MC Dropout: yes, a little to both, as it is enabled in inference and training (and you have to sample multiple times in inference)<br>\n",
    "<br><br>\n",
    "1. _Practice training a deep neural network on the CIFAR10 image dataset:_<br>\n",
    " 1. _Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the Swish activation function._<br>\n",
    " 1. _Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with tf.keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters._<br>\n",
    " 1. _Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model? How does it affect training speed?_<br>\n",
    " 1. _Try replacing Batch Normalization with SELU, and make the necessary adjustments to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.)._<br>\n",
    " 1. _Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout._<br>\n",
    " 1. _Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy._<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48caff51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

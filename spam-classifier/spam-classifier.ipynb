{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74c7fa6",
   "metadata": {},
   "source": [
    "This is my solution to an exercise to make a spam classifier using Apache SpamAssassin’s public datasets, which I am writing to learn about machine learning. The exercise is from \n",
    "_Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition_, by\n",
    "Aurélien Géron.\n",
    "\n",
    "Copyright (C) 2022 Chris March <https://github.com/chrismarch>\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2806268b",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- [Load and Preprocess](#Load-and-Preprocess)\n",
    "- [Validate](#Validate)\n",
    "- [Train](#Train)\n",
    "- [Test](#Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff9411",
   "metadata": {},
   "source": [
    "## Load and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aea9027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "import email\n",
    "from email import policy\n",
    "from io import StringIO\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8738c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    ''' MLStripper by \"Olivier Le Floch\" https://stackoverflow.com/a/925630 '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self): \n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de3ece22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_get_body(b):\n",
    "    ''' \n",
    "    email_get_body by Todor Minikov https://stackoverflow.com/a/32840516\n",
    "    (this seems to be more robust, or at least easier to use without errors than email.Parser.get_body)\n",
    "    '''\n",
    "    body = \"\"\n",
    "    if b.is_multipart():\n",
    "        for part in b.walk():\n",
    "            ctype = part.get_content_type()\n",
    "            cdispo = str(part.get('Content-Disposition'))\n",
    "\n",
    "            # skip any text/plain (txt) attachments\n",
    "            if ctype == 'text/plain' and 'attachment' not in cdispo:\n",
    "                body = part.get_payload(decode=True)  # decode\n",
    "                break\n",
    "    # not multipart - i.e. plain text, no attachments, keeping fingers crossed\n",
    "    else:\n",
    "        body = b.get_payload(decode=True)\n",
    "\n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8910768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_key = 'is_spam'\n",
    "\n",
    "def load_email_files_to_dataframe():\n",
    "    vocabulary = {}\n",
    "    too_common_words = {}\n",
    "    file_paths_to_mail_dicts = {}\n",
    "    file_index = 0\n",
    "    #result = pd.DataFrame(data=file_paths_to_mail_dicts.values(), columns=columns, dtype=pd.SparseDtype(pd.UInt8Dtype())) # has to be nan to start, fill_value=0))     \n",
    "    #df_type = 'Sparse[int]';#pd.SparseDtype(pd.UInt8Dtype())\n",
    "    #result = pd.DataFrame(dtype=df_type) # has to be nan to start, fill_value=0))     \n",
    "\n",
    "    for parent_dir, subdirs, files in os.walk('.'):\n",
    "        #print(parent_dir)\n",
    "        #print(subdirs)\n",
    "        #print('---')\n",
    "        html_regex = re.compile(r\"(<html>|<HTML>)(.*)(<\\/html>|<\\/HTML>)\", re.DOTALL)\n",
    "        hamdir = '_ham' in parent_dir\n",
    "        spamdir = 'spam' in parent_dir\n",
    "        if hamdir or spamdir:\n",
    "            for file in files:\n",
    "                file_index += 1\n",
    "                rel_file_path = os.path.join(parent_dir, file) \n",
    "                #print(rel_file_path)\n",
    "                with open(rel_file_path, 'r', encoding='iso-8859-1') as f:\n",
    "                #with open(rel_file_path, 'rb') as f:\n",
    "                    f_str = f.read()   \n",
    "                    msg = email.message_from_string(f_str, policy=policy.default)\n",
    "                    #msg = email.parser.BytesParser(policy=policy.default).parse(f)\n",
    "                    if file_index % 1000 == 0:\n",
    "                        print(str(file_index))\n",
    "                    subject = msg['subject']\n",
    "                    #print(subject)\n",
    "                    #print(msg['header'])\n",
    "                    #body = msg.get_body(preferencelist=('html', 'plain'))\n",
    "                    #if not body:\n",
    "                    #    continue\n",
    "                    #body = body.get_content()\n",
    "                    body = str(email_get_body(msg))\n",
    "                    #print(body)\n",
    "                    html_match = html_regex.search(body)\n",
    "                    #print(html_match)\n",
    "                    if html_match:\n",
    "                        body = html_match.group(2)\n",
    "                        #print('------------')\n",
    "                        #print(body)\n",
    "                        #print('------------')\n",
    "                    body_strip = strip_tags(body)\n",
    "                    #body_strip = re.sub(r\"[()\\\"\\'-]\", '', body_strip)\n",
    "                    body_strip = re.sub(r\"\\\\n\", ' ', body_strip)\n",
    "                    #print(body_strip)\n",
    "                    mail_tokens = body_strip.split()\n",
    "                    if subject:\n",
    "                        mail_tokens += subject.split()\n",
    "                    mail_dict = {}\n",
    "                    mail_rejected_tokens = []\n",
    "                    for token in mail_tokens:\n",
    "                        if token in mail_dict:\n",
    "                            mail_dict[token] = mail_dict[token] + 1\n",
    "                            too_common_words[token] = 0\n",
    "                            mail_dict.pop(token)\n",
    "                            vocabulary.pop(token)\n",
    "                        elif token not in too_common_words:\n",
    "                            mail_dict[token] = 1\n",
    "                            vocabulary[token] = 0\n",
    "                    mail_dict[spam_key] = 1 if 'spam' in rel_file_path else 0\n",
    "                    file_paths_to_mail_dicts[rel_file_path] = mail_dict\n",
    "                    #mail_df = pd.DataFrame.from_dict(mail_dict, dtype=df_type)\n",
    "                    #mail_df = pd.DataFrame(data=[mail_dict], dtype=df_type)\n",
    "                    #result = pd.concat([result, mail_df])\n",
    "                    #print(mail_tokens)                            \n",
    "                    #print(mail_dict)\n",
    "                    #print(mail_rejected_tokens)\n",
    "                #if file_index > 100:\n",
    "                #break\n",
    "    print('finished loading emails')\n",
    "    \n",
    "    columns = [target_label_col] + list(vocabulary.keys())\n",
    "    \n",
    "    '''\n",
    "    # for each mail, encode\n",
    "    for file_path in file_paths_to_mail_dicts:\n",
    "        mail_dict = file_paths_to_mail_dicts[file_path]\n",
    "        for word in mail_dict:\n",
    "            n_word = mail_dict[word]\n",
    "            if n_word > 255:\n",
    "                #print('!!!!!!', word)\n",
    "                mail_dict.remove(word)\n",
    "        mail_dicts.append(mail_dict)\n",
    "    '''\n",
    "    #mail_dicts = []\n",
    "    #for path in file_paths_to_mail_dicts:\n",
    "    #    mail_dicts.append(file_paths_to_mail_dicts[path])\n",
    "    #print('finished constructing rows')\n",
    "    #df_type = 'Sparse[int]'#pd.SparseDtype(pd.UInt8Dtype())\n",
    "    #return pd.DataFrame.from_dict(row_dict, orient='index',dtype=pd.SparseDtype(np.uint8))\n",
    "    # TODO convert to zero fill sparse from nan sparse and serialize zero fill sparse\n",
    "    #nan_df = pd.DataFrame(data=mail_dicts, columns=columns, dtype=pd.SparseDtype(np.uint8)) # has to be nan to start, fill_value=0))\n",
    "    #return pd.DataFrame(data=nan_df, dtype=pd.SparseDtype(np.uint8, fill_value=0))\n",
    "    #return pd.DataFrame(data=list(file_paths_to_mail_dicts.values()), columns=columns, dtype='Sparse[float]')\n",
    "        #\"    nan_df = pd.DataFrame(data=mail_dicts, columns=columns, dtype=pd.SparseDtype(np.dtype('float64'))) # has to be nan to start, fill_value=0))\\n\",\n",
    "\n",
    "    #return pd.DataFrame(data=mail_dicts, columns=columns, dtype=pd.SparseDtype(np.dtype('float64')))\n",
    "    return pd.DataFrame(data=file_paths_to_mail_dicts.values(), columns=columns, dtype='Sparse[int]')#pd.SparseDtype(pd.UInt8Dtype()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9ff47828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "finished loading emails\n",
      "0.0003544377510030051\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10751 entries, 0 to 10750\n",
      "Columns: 131171 entries, is_spam to https://listman.redhat.com/\n",
      "dtypes: Sparse[UInt8, <NA>](131171)\n",
      "memory usage: 2.9 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# warning: this can take a couple/few minutes    \n",
    "all_data = load_email_files_to_dataframe()\n",
    "print(all_data.sparse.density)\n",
    "print(all_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "527a69ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spamham.pkl']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(all_data, \"spamham.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ac993293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10751 entries, 0 to 10750\n",
      "Columns: 131170 entries, b\"Friend,Now to https://listman.redhat.com/\n",
      "dtypes: Sparse[UInt8, <NA>](131170)\n",
      "memory usage: 2.8 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#all_data = None\n",
    "all_data = joblib.load(\"spamham.pkl\")\n",
    "print(all_data.info())\n",
    "#all_data = pd.DataFrame(data=nan_data, dtype=pd.SparseDtype(np.dtype('float64'), fill_value=0))\n",
    "#nan_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b738a287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10751 entries, 0 to 10750\n",
      "Columns: 131171 entries, is_spam to https://listman.redhat.com/\n",
      "dtypes: Sparse[UInt8, <NA>](131171)\n",
      "memory usage: 2.9 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(target_label_col in all_data)\n",
    "print(all_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "33f0110c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10751 entries, 0 to 10750\n",
      "Columns: 131171 entries, is_spam to https://listman.redhat.com/\n",
      "dtypes: Sparse[int64, 0](131171)\n",
      "memory usage: 5.7 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.DataFrame(data=all_data, dtype='Sparse[int]')\n",
    "joblib.dump(all_data, \"spamham.pkl\")\n",
    "print(all_dense.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3f116d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "974009b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003544377510030051\n"
     ]
    }
   ],
   "source": [
    "print(all_data.sparse.density)\n",
    "target_label_col = spam_key\n",
    "y_all = all_data[target_label_col]\n",
    "for train_indexes, test_indexes in split.split(all_data, y_all):\n",
    "    strat_train_set = all_data.loc[train_indexes]\n",
    "    strat_test_set = all_data.loc[test_indexes]\n",
    "\n",
    "#print(list(strat_train_set))\n",
    "X_train = strat_train_set.drop(target_label_col, axis=1)\n",
    "y_train = strat_train_set[target_label_col].copy()\n",
    "strat_train_set = None\n",
    "\n",
    "# test data split from train.csv, since test.csv has no labels\n",
    "X_test = strat_test_set.drop(target_label_col, axis=1)\n",
    "y_test = strat_test_set[target_label_col].copy()\n",
    "strat_test_set = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b88bbb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8600 entries, 20 to 10180\n",
      "Columns: 131170 entries, b\"Friend,Now to https://listman.redhat.com/\n",
      "dtypes: Sparse[int64, 0](131170)\n",
      "memory usage: 4.6 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2151 entries, 2881 to 4362\n",
      "Columns: 131170 entries, b\"Friend,Now to https://listman.redhat.com/\n",
      "dtypes: Sparse[int64, 0](131170)\n",
      "memory usage: 1.1 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(X_train.info())\n",
    "print(X_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231fe4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def get_largest_user_globals():\n",
    "    ''' Adapted from https://stackoverflow.com/a/40997868 by Abdou '''\n",
    "    \n",
    "    # These are the usual ipython objects, including this one you are creating\n",
    "    ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "    # Get a sorted list of the objects and their sizes\n",
    "    sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c8885a",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1cd109d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linsvc_clf = LinearSVC(C=.1)\n",
    "mnb_clf = MultinomialNB()                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9014667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MultinomialNB\n",
      "Mean: 0.9856976744186046\n",
      "Standard deviation: 0.0022845212446963464\n"
     ]
    }
   ],
   "source": [
    "def display_scores(estimator, scores):\n",
    "    print(\"\\n\")\n",
    "    print(type(estimator).__name__)\n",
    "    #print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "        \n",
    "def display_estimator_cv_scores(estimator, X, y): \n",
    "    scores = cross_val_score(estimator, X, y, cv=5)\n",
    "    display_scores(estimator, scores)\n",
    "\n",
    "def print_cv_scores(estimators, X, y):\n",
    "    scores_and_estimators = []\n",
    "    for e in estimators:\n",
    "        scores = cross_val_score(e, X, y, cv=5)\n",
    "        scores_and_estimators.append((scores.mean(), scores, e))\n",
    "        \n",
    "    scores_and_estimators.sort(key = lambda x: x[0], reverse=True)\n",
    "    for mean, scores, e in scores_and_estimators:\n",
    "        display_scores(e, scores)\n",
    " \n",
    "'''\n",
    "words only, no replacement\n",
    "MultinomialNB\n",
    "Mean: 0.9759302325581395\n",
    "Standard deviation: 0.0023139242723409837\n",
    "\n",
    "unfiltered tokens (no headers or html tags, #body_strip = re.sub(r\"[()\\\"\\'-]\", '', body_strip))\n",
    "MultinomialNB\n",
    "Mean: 0.9853488372093022\n",
    "Standard deviation: 0.002156655464068763\n",
    "\n",
    "unfiltered tokens (no headers or html tags,\n",
    "MultinomialNB\n",
    "Mean: 0.9856976744186046\n",
    "Standard deviation: 0.0022845212446963464\n",
    "'''\n",
    "\n",
    "estimators = [mnb_clf]#, linsvc_clf]\n",
    "print_cv_scores(estimators, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ceecb1",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6ea2a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "class VocabSimplifier(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, to_lower=False, strip_punctuation=False, replace_urls=True, replace_numbers=False): # no *args or **kargs\n",
    "        self.to_lower = to_lower\n",
    "        self.strip_punctuation = strip_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "                \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def apply_simplifications(self, word):\n",
    "        if (word == spam_key):\n",
    "            return word\n",
    "        \n",
    "        if (self.to_lower):\n",
    "            word = word.lower()\n",
    "            \n",
    "        if (self.strip_punctuation):\n",
    "            word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "            \n",
    "        if (self.replace_urls):\n",
    "            word = re.sub(r'http\\S+', '_URL_', word)\n",
    "            \n",
    "        if (self.replace_numbers):\n",
    "            word = re.sub(r'[0-9]+', '_N_', word)\n",
    "        return word\n",
    "            \n",
    "    def transform(self, X : pd.DataFrame):\n",
    "        #print(\"VocabSimplifier.transform in.shape:\", X)\n",
    "        X = X.copy()\n",
    "        cols_to_drop = []\n",
    "        simplified_vocab = {}\n",
    "        vocab = list(X)\n",
    "        n_vocab = len(vocab)\n",
    "        progress_size = n_vocab/10\n",
    "        for i, word in enumerate(vocab):\n",
    "            if i % progress_size == 0:\n",
    "                print(i, '/', n_vocab)\n",
    "#            print(word)\n",
    "            w = self.apply_simplifications(word)\n",
    "#            print(i, w, word)\n",
    "            if not w in simplified_vocab:\n",
    "                simplified_vocab[w] = [word]\n",
    "            else:\n",
    "                #w_i = int(simplified_vocab[w])\n",
    "#                print((\"COLUMN DEL\", w, i, word))\n",
    "                simplified_vocab[w].append(word)\n",
    "                cols_to_drop.append(word)\n",
    "\n",
    "        print(n_vocab, '/', n_vocab)\n",
    "\n",
    "        for similar_words in simplified_vocab.values():      \n",
    "            if len(similar_words) > 1:\n",
    "                main_col_for_w = similar_words[0]        \n",
    "                #print(main_col_for_w)\n",
    "                sparse_type = X.dtypes[main_col_for_w]\n",
    "                '''\n",
    "                print(X.dtypes[main_col_for_w])\n",
    "                X[main_col_for_w] = X[main_col_for_w].sparse.to_dense()\n",
    "                print(X.dtypes[main_col_for_w])\n",
    "                for word in similar_words:\n",
    "                    if word != main_col_for_w:\n",
    "                        print(X.dtypes[main_col_for_w])\n",
    "                        X[main_col_for_w] += X[word].sparse.to_dense()                        \n",
    "                '''\n",
    "                X[main_col_for_w] = X[similar_words].sum(axis=1).astype(sparse_type)\n",
    "\n",
    "        #print(\"VocabSimplifier.transform cols_to_drop:\", cols_to_drop)\n",
    "        print('VocabSimplifier dropping columns')\n",
    "        X.drop(columns=cols_to_drop, inplace=True)\n",
    "        #print(\"VocabSimplifier.transform out.shape:\", X.info())\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c49ffc1",
   "metadata": {},
   "source": [
    "hyperparameters to your preparation pipeline to control whether or not to \n",
    "- (strip off email headers)\n",
    "- convert each email to lowercase, \n",
    "- remove punctuation, \n",
    "- replace all URLs with “URL,” \n",
    "- replace all numbers with “NUMBER,” \n",
    "- (or even perform stemming (i.e., trim off word endings; there are Python libraries available to do this).)\n",
    "\n",
    "Finally, try out several classifiers and see if you can build a great spam classifier, with both high recall and high precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "4876be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_regex = re.compile(r\"^[(]?((mp3|MP3)[sS]?|[a-zA-Z]+|[a-zA-Z]+[-\\/]?[a-zA-Z]+|[a-zA-Z]+[-\\/]?[a-zA-Z]+[']?[a-zA-Z]+)([)]?[!?]+|[!?]+[)]?|[.,;:)]?)$\")\n",
    "\n",
    "mnb_param_grid = [\n",
    "    {'alpha': [0.0001, .5, 1], 'fit_prior': [True, False] }\n",
    "  ]\n",
    "\n",
    "grid_search = GridSearchCV(mnb_clf, mnb_param_grid, cv=5,\n",
    "                           #return_train_score=True, \n",
    "                           verbose=2)\n",
    "\n",
    "#col_trans.named_transformers_[\"cat\"].handle_unknown = 'ignore' # for dropping attributes\n",
    "\n",
    "main_pipeline = Pipeline([\n",
    "    ('simple', VocabSimplifier()),\n",
    " #   ('dropper', AttributesDropper()),\n",
    "    #('mnb', MultinomialNB()),\n",
    "    ('grid', grid_search)\n",
    "])\n",
    "\n",
    "param_main = [\n",
    "    {'simple__to_lower': [False, True]},\n",
    "    {'simple__strip_punctuation': [False, True]},\n",
    "    {'simple__replace_urls': [False, True]},\n",
    "    {'simple__replace_numbers': [False, True]}\n",
    "  ]\n",
    "\n",
    "full_pipeline = GridSearchCV(main_pipeline, param_main, cv=5,\n",
    "                           verbose=2)#, error_score='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c92fc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 131170\n",
      "13117 / 131170\n",
      "26234 / 131170\n",
      "39351 / 131170\n",
      "52468 / 131170\n",
      "65585 / 131170\n",
      "78702 / 131170\n",
      "91819 / 131170\n",
      "104936 / 131170\n",
      "118053 / 131170\n",
      "131170 / 131170\n",
      "VocabSimplifier dropping columns\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "pipe_out = main_pipeline.fit(X_train, y_train)\n",
    "print(pipe_out.best_estimator_.steps[1][1].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_out = full_pipeline.fit(X_train, y_train)\n",
    "#pipe_out = main_pipeline.fit(X_train, y_train)\n",
    "#print(grid_search.best_params_)\n",
    "print(full_pipeline.best_params_)\n",
    "print(full_pipeline.best_estimator_.steps[2][1].best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49646249",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5a691f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 131170\n",
      "13117 / 131170\n",
      "26234 / 131170\n",
      "39351 / 131170\n",
      "52468 / 131170\n",
      "65585 / 131170\n",
      "78702 / 131170\n",
      "91819 / 131170\n",
      "104936 / 131170\n",
      "118053 / 131170\n",
      "http://inglesa.net/unsub.php?client=atomicDOT\n",
      "(http://admanmail.com/subscription.asp?em=JM@NETNOTEINC.COM&l=SGO)\n",
      "window.open(\"http://www.ouweilighting.com\");\n",
      "\"http://www.radisson-chicago.com\"\n",
      ">http://www.thaiworkathome.com/unsubscribe.php\n",
      "url(http://images.lockergnome.com/images/issue/top-right.gif);\n",
      "\\thttp://use.perl.org/my/messages/\n",
      "b'http://www.rebackee.com/cursos2/contraloria.htm\n",
      "Websie:http://www.wjjzzs.com\\r\n",
      "open(\"http://www.pointcom.com\",\"_top\");}\n",
      "b\"http://www.nme.com/news/102774.htm\n",
      "\\'http0:python\\'\n",
      ">>http://www.frogstone.net/Cafe/CafeForteana.html\n",
      "[1]http://www.theperlreview.com\n",
      "0.9739921976592978\n",
      "0.9855263157894737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9842730411939666"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "#X_test_tr = col_trans.fit_transform(X_test)\n",
    "#print(list(X_train))\n",
    "#print(list(X_test))\n",
    "\n",
    "# forest\n",
    "# 0.785314498933902\n",
    "# 0.7706711343254163 drop age\n",
    "\n",
    "# knn\n",
    "# 0.7795566502463054\n",
    "\n",
    "# sgd\n",
    "# 0.777129750982962\n",
    "\n",
    "y_test_predict = pipe_out.predict(X_test)\n",
    "print(precision_score(y_test, y_test_predict)) # first priority for spam classifier\n",
    "print(recall_score(y_test, y_test_predict))    # second priority\n",
    "score = f1_score(y_test, y_test_predict, average=\"macro\")\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77153405",
   "metadata": {},
   "source": [
    "# Chapter 10 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097f07a",
   "metadata": {},
   "source": [
    "1. _(TensorFlow Playground)_<br>\n",
    "<br>\n",
    "Done.<br>\n",
    "<br>\n",
    "1. _Draw an ANN using the original artificial neurons (like the ones in Figure 10-3) that computes A ⊕ B (where ⊕ represents the XOR operation). Hint: A ⊕ B = (A ∧ ¬ B) ∨ (¬ A ∧ B)._<br>\n",
    "<br>\n",
    "<br><br>\n",
    "1. _Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of threshold logic units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?_<br>\n",
    "<br>\n",
    "<br><br>\n",
    "1. _Why was the sigmoid activation function a key ingredient in training the first MLPs?_<br>\n",
    "<br>\n",
    "<br><br>\n",
    "1. _Name three popular activation functions. Can you draw them?_<br>\n",
    "<br>\n",
    "<br><br>\n",
    "1. _Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function._\n",
    "    * _What is the shape of the input matrix X?_\n",
    "    * _What are the shapes of the hidden layer’s weight matrix Wh and bias vector bh?_\n",
    "    * _What are the shapes of the output layer’s weight matrix Wo and bias vector bo?_\n",
    "    * _What is the shape of the network’s output matrix Y?_\n",
    "    * _Write the equation that computes the network’s output matrix Y as a function of X, Wh, bh, Wo, and bo._<br>\n",
    "<br>\n",
    "<br><br>\n",
    "1. _How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, and which activation function should you use? What about for getting your network to predict housing prices, as in Chapter 2?_<br>\n",
    "<br>\n",
    "<br><br>\n",
    "1. _What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?_<br>\n",
    "<br>\n",
    "Backpropagation is the step after forward propagation to determine the contributions from the layers of the network to the loss error.\n",
    "TODO Reverse-mode autodiff is the processes of automatically determining the gradients\n",
    "<br><br>\n",
    "1. _Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?_<br>\n",
    "<br>\n",
    "<br><br>\n",
    "1. _Train a deep MLP on the MNIST dataset (you can load it using tf.keras.datasets.mnist.load_data(). See if you can get over 98% accuracy by manually tuning the hyperparameters. Try searching for the optimal learning rate by using the approach presented in this chapter (i.e., by growing the learning rate exponentially, plotting the loss, and finding the point where the loss shoots up). Next, try tuning the hyperparameters using Keras Tuner with all the bells and whistles—save checkpoints, use early stopping, and plot learning curves using TensorBoard._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62dfcc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
